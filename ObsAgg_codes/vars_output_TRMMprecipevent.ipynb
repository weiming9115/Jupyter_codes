{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### composite analysis for thermodynamic variabilities\n",
    "\n",
    "1. create dataset <br>\n",
    "2. [read data](#readdata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import xarray as xr\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, timedelta\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeat\n",
    "from cartopy.util import add_cyclic_point\n",
    "from cartopy.mpl.gridliner import LONGITUDE_FORMATTER, LATITUDE_FORMATTER\n",
    "\n",
    "import warnings\n",
    "import psutil\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.dates import DateFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ISCCP_dir = '/w2-data2/willytsai/ISCCP/hgg/'\n",
    "TRMM_dir = '/data2/willytsai/TRMM_3hr/TRMM/2009_2018'\n",
    "#MERRA2_dir = '/w2-data2/willytsai/MERRA2/'\n",
    "#MERRA2_3d_dir = '/w2-data/willytsai/'\n",
    "#ERA5_dir = '/w2-data/willytsai/ERA5/'\n",
    "#Gridsat_dir = '/w2-data2/willytsai/gridsat_BT/remap_0.25deg/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # MERRA2 dataset 2014-2015, hourly in a single file\n",
    "# os.chdir(MERRA2_dir+'inst_2d_hourly/')\n",
    "# files = sorted(glob('MERRA2_400.inst1_2d_asm_Nx.201*'))\n",
    "# data = xr.open_mfdataset(files)\n",
    "# data = data.resample(time='3H').nearest() # CWV\n",
    "# cwv_xr = data.TQV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%time \n",
    "#os.chdir(ERA5_dir)\n",
    "#files = sorted(glob('*.nc'))\n",
    "#data = xr.open_mfdataset(files)\n",
    "#cwv_era_xr = data.tcwv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "date = []; date_begin = datetime(2014,1,1)\n",
    "for t in range(14608):\n",
    "    tmp = date_begin + t*timedelta(hours=3)\n",
    "    date.append(tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 56s, sys: 1min 9s, total: 4min 5s\n",
      "Wall time: 6min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TRMM3B42 dataset 2014, 3hrly separate\n",
    "yr = ['2009_2018']\n",
    "prec_3hr = np.nan*np.zeros((len(date),241,1440))\n",
    "\n",
    "n=1\n",
    "os.chdir(TRMM_dir)\n",
    "files = sorted(glob('*2014*')+glob('*2015*')+glob('*2016*')+glob('*2017*')+glob('*2018*'))\n",
    "for file in files:\n",
    "    data = xr.open_dataset(file).precipitation        \n",
    "    prec_3hr[n-1,:,:] = data.values.swapaxes(0,1)\n",
    "    n+=1\n",
    "lon = data.nlon; lat = data.nlat\n",
    "prec_xr = xr.DataArray(prec_3hr,coords=[date,lat,lon],dims=['time','lat','lon']); del prec_3hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # MERRA2 reanalysis 2014, 3hrly \n",
    "\n",
    "# os.chdir(MERRA2_3d_dir+'TQUV_3hr/')\n",
    "# files = sorted(glob('MERRA2_400.inst3_3d_asm_Np.2014*')\n",
    "#                + glob('MERRA2_400.inst3_3d_asm_Np.2015*'))\n",
    "# data = xr.open_mfdataset(files)\n",
    "# T_xr = data.T   \n",
    "# q_xr = data.QV  \n",
    "# u_xr = data.U\n",
    "# v_xr = data.V\n",
    "\n",
    "# # wind shear\n",
    "# llwsz_xr = np.sqrt((u_xr.sel(lev=900)-u_xr.sel(lev=700))**2 + \n",
    "#                   (v_xr.sel(lev=900)-v_xr.sel(lev=700))**2)\n",
    "\n",
    "# os.chdir(MERRA2_3d_dir+'omega_3hr/')\n",
    "# files = sorted(glob('MERRA2_400.inst3_3d_asm_Np.2014*')\n",
    "#                + glob('MERRA2_400.inst3_3d_asm_Np.2015*'))\n",
    "# data = xr.open_mfdataset(files)\n",
    "# Omega_xr = data.OMEGA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # MERRA2 OLR dataset 2014-2015, hourly in a single file\n",
    "# os.chdir(MERRA2_dir+'inst2d_OLR')\n",
    "# files = sorted(glob('MERRA2_400.tavg1_2d_rad_Nx.2014*')\n",
    "#               + glob('MERRA2_400.tavg1_2d_rad_Nx.2015*'))\n",
    "# data = xr.open_mfdataset(files)\n",
    "# data = data.resample(time='3H').nearest() # CWV\n",
    "# olr_xr = data.LWTUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # Gridsat-BT dataset 2014, 2015 3hourly\n",
    "# yr = ['2014','2015']\n",
    "# bt_3hr = np.nan*np.zeros((len(date),241,1440))\n",
    "\n",
    "# n=1\n",
    "# for y in yr:\n",
    "#     os.chdir(Gridsat_dir+y)\n",
    "#     files = sorted(glob('GRIDSAT-B1.*'))\n",
    "#     for file in files:\n",
    "#         data = xr.open_dataset(file)\n",
    "#         bt_3hr[n-1,:,:] = data.irwin_cdr\n",
    "#         n+=1\n",
    "# lon = data.lon; lat = data.lat\n",
    "# bt_xr = xr.DataArray(bt_3hr,coords=[date,lat,lon],dims=['time','lat','lon']); del bt_3hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # derive Relative hunidity\n",
    "# lev = T_xr.lev\n",
    "# es = 6.1094*np.exp(17.625*(T_xr-273)/(T_xr-273+243.04))\n",
    "# p = (q_xr/q_xr)*lev\n",
    "# qs = es/(p-es)*0.622\n",
    "# rh_xr = q_xr/qs*100 # relative humidity [%] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# cldamt_type_3hr = np.nan*np.zeros((len(date),18,180,360))\n",
    "# cldamt_3hr = np.nan*np.zeros((len(date),180,360))\n",
    "# # ISCCP dataset 2014, 3hourly in a single file\n",
    "# yr = ['2014','2015'] # year\n",
    "# mn = ['01','02','03','04','05','06','07','08',\n",
    "#      '09','10','11','12'] # month\n",
    "\n",
    "# n=1\n",
    "# for y in yr:\n",
    "#     for m in mn:\n",
    "#         os.chdir(ISCCP_dir+y+m)\n",
    "#         files = sorted(glob('*.nc'))\n",
    "        \n",
    "#         for file in files:\n",
    "#             data = xr.open_dataset(file).cldamt_types # cloud amount [%]\n",
    "#             data2 = xr.open_dataset(file).cldamt\n",
    "#             tmp = data; tmp2 = data2\n",
    "#             cldamt_type_3hr[n-1,:,:,:] = tmp\n",
    "#             cldamt_3hr[n-1,:,:] = tmp2\n",
    "#             n+=1\n",
    "# lat = tmp.lat; lon = tmp.lon\n",
    "# print(n-1)\n",
    "# cldamt_type_xr = xr.DataArray(cldamt_type_3hr,coords=[date,range(18),lat,lon],dims=['time','type','lat','lon']); del cldamt_type_3hr\n",
    "# cldamt_xr = xr.DataArray(cldamt_3hr,coords=[date,lat,lon],dims=['time','lat','lon']); del cldamt_3hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lat = cldamt_type_xr.lat\n",
    "# lon = cldamt_type_xr.lon\n",
    "# # 0+3 shallow cumulus, 2+5 stratoform, 14+17 deep convection \n",
    "# cldamt_cu = (cldamt_type_xr[:,0,:,:]+cldamt_type_xr[:,3,:,:]).values\n",
    "# cldamt_cu[cldamt_cu>100] = np.nan; cldamt_cu[cldamt_cu< 0] = np.nan;\n",
    "# cu_xr = xr.DataArray(cldamt_cu,coords=[date,lat,lon],dims=['time','lat','lon']); del cldamt_cu\n",
    "\n",
    "# cldamt_st = (cldamt_type_xr[:,13,:,:]+cldamt_type_xr[:,16,:,:]).values\n",
    "# cldamt_st[cldamt_st>100] = np.nan; cldamt_st[cldamt_st< 0] = np.nan;\n",
    "# st_xr = xr.DataArray(cldamt_st,coords=[date,lat,lon],dims=['time','lat','lon']); del cldamt_st\n",
    "\n",
    "# cldamt_dc = (cldamt_type_xr[:,14,:,:]+cldamt_type_xr[:,17,:,:]).values\n",
    "# cldamt_dc[cldamt_dc>100] = np.nan; cldamt_dc[cldamt_dc< 0] = np.nan;\n",
    "# dc_xr = xr.DataArray(cldamt_dc,coords=[date,lat,lon],dims=['time','lat','lon']); del cldamt_dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fft_filter(data,filtered_day,span_days):\n",
    "    \n",
    "    Vp = (data - data.mean()) # olr anomaly\n",
    "    Vspec = np.fft.fft(Vp) \n",
    "\n",
    "    NT=len(data)\n",
    "    freqs = np.array(range(NT))   # This gets the left end correct\n",
    "    reversed_freqs = freqs[::-1]  # Reverse the array \n",
    "    reversed_freqs[0:int(NT/2)] = freqs[1:int(NT/2)+1] # Fix the low end of the reversed array\n",
    "    freqs = reversed_freqs[::-1]  # reverse it back \n",
    "    \n",
    "    # filtering and reconstructing data\n",
    "    Vspec_filt = Vspec.copy()\n",
    "    Vspec_filt[np.where(span_days/freqs < filtered_day)] = 0 # 60/freqs [day], filter out periods less than 2 day \n",
    "    Vp_recon = np.fft.ifft(Vspec_filt).real\n",
    "    \n",
    "    return Vp_recon + data.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def readdata(lat_s,lat_n,lon_w,lon_e,start_date,end_date):\n",
    "    \n",
    "    global prec_xr#,cwv_era_xr,cwv_xr,dc_xr,cu_xr,st_xr,cldamt_xr\n",
    "#    global T_xr,q_xr,u_xr,v_xr,Omega_xr,rh_xr,olr_xr,bt_xr\n",
    "    \n",
    "    prec_sm = prec_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "                      time=slice(start_date,end_date))\n",
    "#     cwv_sm = cwv_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "#     cwv_era_sm = cwv_era_xr.sel(latitude=slice(lat_n,lat_s),longitude=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "#     dc_sm = dc_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "#     cu_sm = cu_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "#     st_sm = st_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "#     cldamt_sm = cldamt_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "    \n",
    "#     T_sm = T_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "#     q_sm = q_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "#     u_sm = u_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "#     v_sm = v_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "#     omega_sm = Omega_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "#     rh_sm = rh_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "#     olr_sm = olr_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "#     bt_sm = bt_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "#     llwsz_sm = llwsz_xr.sel(lat=slice(lat_s,lat_n),lon=slice(lon_w,lon_e),\n",
    "#                       time=slice(start_date,end_date))\n",
    "\n",
    "    return prec_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def org_index(date,prec_sm,olr_sm,bt_sm):\n",
    "    \n",
    "    N_ps = np.zeros(prec_sm.shape[0]) # OLR_based\n",
    "    N_psT = np.zeros(prec_sm.shape[0]) # BT_based\n",
    "    N_prec = np.zeros(prec_sm.shape[0]) # prec_based\n",
    "    cldfrac = np.zeros(prec_sm.shape[0]) # cldfrac over domain by OLR criteria\n",
    "    precfrac = np.zeros(prec_sm.shape[0]) # cldfrac over domain by OLR criteria\n",
    "    SCAI_BT = np.zeros(prec_sm.shape[0]) # cldfrac over domain by OLR criteria\n",
    "    SCAI_p = np.zeros(prec_sm.shape[0]) # cldfrac over domain by OLR criteria\n",
    "\n",
    "    \n",
    "    # N and SCAI indices\n",
    "    for t in range(prec_sm.shape[0]):\n",
    "        label_obj, N = scai.labeled_obj(olr_sm[t,:,:].values,cri=180,flag=2) # less than 220 W/m^2\n",
    "        N_ps[t] = N\n",
    "\n",
    "    for t in range(prec_sm.shape[0]):\n",
    "        label_obj, N = scai.labeled_obj(bt_sm[t,:,:].values,cri=240,flag=2) # less than 220 W/m^2\n",
    "        SCAI_BT[t] = scai.SCAI_calc(label_obj,N,dx=0.25*100000,L=10*100000)\n",
    "        N_psT[t] = N\n",
    "    \n",
    "    for t in range(prec_sm.shape[0]):\n",
    "        label_obj, N = scai.labeled_obj(prec_sm[t,:,:].values,cri=1,flag=1) # larger than 1mm/day\n",
    "        SCAI_p[t] = scai.SCAI_calc(label_obj,N,dx=0.25*100000,L=10*100000)    \n",
    "        N_prec[t] = N\n",
    "\n",
    "    N_ps = xr.DataArray(N_ps,coords=[date],dims=['time'])\n",
    "    N_psT = xr.DataArray(N_psT,coords=[date],dims=['time'])\n",
    "    N_prec = xr.DataArray(N_prec,coords=[date],dims=['time'])\n",
    "    \n",
    "    \n",
    "    # cloud fraction (by OLR < 180)\n",
    "    for t in range(prec_sm.shape[0]):\n",
    "        tmp = len(np.where(bt_sm[t,:,:].values<240)[0])/(bt_sm[t,:,:].shape[0]*bt_sm[t,:,:].shape[1])\n",
    "        cldfrac[t] = tmp\n",
    "    cldfrac = xr.DataArray(cldfrac,coords=[date],dims=['time'])\n",
    "    \n",
    "    # prec fraction (by prec > 1mm/hr)\n",
    "    for t in range(prec_sm.shape[0]):\n",
    "        tmp = len(np.where(prec_sm[t,:,:].values>1)[0])/(prec_sm[t,:,:].shape[0]*prec_sm[t,:,:].shape[1])\n",
    "        precfrac[t] = tmp\n",
    "    precfrac = xr.DataArray(precfrac,coords=[date],dims=['time'])\n",
    "    \n",
    "    return (N_ps, N_psT, N_prec, SCAI_BT, SCAI_p, cldfrac, precfrac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def running_mean(y,window_N):\n",
    "    y_avg = np.zeros(len(y))\n",
    "    avg_mask = np.ones(window_N) / window_N\n",
    "    y_avg = np.convolve(y, avg_mask, 'same')\n",
    "    return y_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Composite time series for thermodynamic variability based on degree of organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### number of precipitating systems \n",
    "os.chdir('/data2/willytsai/python_module')\n",
    "import SCAI_calc4obj as scai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Domain and Time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-7.5 -5.  -2.5  0.   2.5  5.   7.5]\n",
      "[  52.5   55.    57.5   60.    62.5   65.    67.5   70.    72.5   75.\n",
      "   77.5   80.    82.5   85.    87.5  152.5  155.   157.5  160.   162.5\n",
      "  165.   167.5  170.   172.5  175.   177.5 -177.5 -175.  -172.5 -170.\n",
      " -167.5 -165.  -162.5 -160.  -157.5 -155.  -152.5 -150.  -147.5 -145.\n",
      " -142.5 -140.  -137.5 -135.  -132.5 -130.  -127.5 -125.  -122.5 -120.\n",
      " -117.5 -115.  -112.5 -110.  -107.5 -105.  -102.5 -100.   -97.5  -95.\n",
      "  -92.5]\n"
     ]
    }
   ],
   "source": [
    "# time window\n",
    "start_date = datetime(2014,1,1)\n",
    "end_date = datetime(2019,1,1)\n",
    "\n",
    "# INDO = lat(-10,10), lon(65,75)\n",
    "lat_s,lat_n = -10,10\n",
    "lon_w,lon_e = 50,90\n",
    "#lon_w,lon_e = 60,70\n",
    "deg_box = 10 \n",
    "overlap = 2.5\n",
    "\n",
    "lat_bnd = np.linspace(lat_s,lat_n,int((lat_n-lat_s)/overlap)+1) \n",
    "lon_bnd = np.linspace(lon_w,lon_e,int((lon_e-lon_w)/overlap)+1)\n",
    "lat_grid = lat_bnd[1:-1]\n",
    "lon_grid = lon_bnd[1:-1]\n",
    "\n",
    "# W_Pacific = lat(-10,10), lon(150,180)\n",
    "lon_w,lon_e = 150,180\n",
    "\n",
    "lon_bnd = np.linspace(lon_w,lon_e,int((lon_e-lon_w)/overlap)+1)\n",
    "lon_grid = np.concatenate((lon_grid,lon_bnd[1:-1]))\n",
    "\n",
    "# E_Pacific = lat(-10,10), lon(150,180)\n",
    "lon_w,lon_e = -180,-90\n",
    "\n",
    "lon_bnd = np.linspace(lon_w,lon_e,int((lon_e-lon_w)/overlap)+1)\n",
    "lon_grid = np.concatenate((lon_grid,lon_bnd[1:-1]))\n",
    "print(lat_grid)\n",
    "print(lon_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Main frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_table = ['prec_sm', 'cwv_sm', 'cwv_era_sm', 'dc_sm', 'cu_sm', 'st_sm', 'cldamt_sm'\n",
    "           ,'T_sm', 'q_sm', 'u_sm', 'v_sm', 'omega_sm', 'rh_sm', 'olr_sm', 'bt_sm', 'llwsz_sm']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 28s, sys: 52.5 s, total: 2min 20s\n",
      "Wall time: 2min 20s\n"
     ]
    }
   ],
   "source": [
    "%%time \n",
    "\n",
    "var_list = [] # total events \n",
    "\n",
    "for lat in lat_grid:\n",
    "    for lon in lon_grid:\n",
    "        metadata = readdata(lat-deg_box/2,lat+deg_box/2,lon-deg_box/2,lon+deg_box/2,\n",
    "                    start_date,end_date)\n",
    "        #(prec_sm, cwv_sm, cwv_era_sm, dc_sm, cu_sm, st_sm, cldamt_sm, T_sm,\n",
    "        #    q_sm, u_sm, v_sm, omega_sm, rh_sm, olr_sm, bt_sm, llwsz_sm) = metadata  \n",
    "        \n",
    "        prec_sm = metadata\n",
    "        \n",
    "        #(N_ps, N_psT, N_prec, SCAI_BT, SCAI_p, cldfrac, precfrac) = org_index(date,prec_sm,olr_sm,bt_sm)\n",
    "        # calculate domain-average variables for 2D\n",
    "        prec_dmean = (prec_sm.mean(axis=1)).mean(axis=1).values         \n",
    "        cwv_dmean = 0 #(cwv_sm.mean(axis=1)).mean(axis=1).values         \n",
    "        dc_dmean = 0 #(dc_sm.mean(axis=1)).mean(axis=1).values         \n",
    "        cu_dmean = 0 #(cu_sm.mean(axis=1)).mean(axis=1).values         \n",
    "        st_dmean = 0 #(st_sm.mean(axis=1)).mean(axis=1).values         \n",
    "        cldamt_dmean = 0 #(cldamt_sm.mean(axis=1)).mean(axis=1).values     \n",
    "        olr_dmean = 0 #(olr_sm.mean(axis=1)).mean(axis=1).values  \n",
    "        bt_dmean = 0 #(bt_sm.mean(axis=1)).mean(axis=1).values  \n",
    "        llwsz_dmean = 0 #(llwsz_sm.mean(axis=1)).mean(axis=1).values  \n",
    "                \n",
    "        # July 20, 2020 1-day running mean \n",
    "        #prec_dmean = running_mean(prec_dmean,8)\n",
    "        \n",
    "        # define convective events by rainfall\n",
    "        t_event = []\n",
    "\n",
    "        t_all = np.where(prec_dmean*24>5)[0] # averaged rainfall > 5 mm/day\n",
    "        t_all = t_all[np.logical_and(t_all > 16,t_all < len(date)-17)] \n",
    "        for t in t_all:\n",
    "            p_window = prec_dmean[t-16:t+17]\n",
    "            if p_window.max() == p_window[16]: # prec max within 3d\n",
    "                t_event.append(t)        \n",
    "        t_event = np.asarray(t_event)\n",
    "        \n",
    "        # make sample list containing variables of subdomains        \n",
    "        for t in t_event:\n",
    "            tmp = [date[t],lat,lon,prec_dmean[t]*24,0,0, \\\n",
    "                           0,0,0 \\\n",
    "                           ,0,0,0, \\\n",
    "                           0,0,0 \\\n",
    "                           ,0,0,0,0, \\\n",
    "                            0]\n",
    "            var_list.append(tmp)   \n",
    "\n",
    "# convert list to array            \n",
    "var_list = np.asarray(var_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name = ['date','lat','lon','prec_dmean','cwv_dmean_MERRA2','cwv_dmean_ERA',\n",
    "           'dc_cldamt_dmean','cu_cldamt_dmean','st_cldamt_dmean','cldamt_dmean',\n",
    "           'olr_dmean','bt_dmean','N_olr','N_BT','N_prec','SCAI_BT','SCAI_p',\n",
    "           'cldfrac_BT','precfrac','llws_dmean'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save variable list\n",
    "np.savetxt('/data/willytsai/ISCCP/var_name_2014_2018_10deg_4ds.dat',var_name,fmt='%16s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save time \n",
    "time_str = np.zeros(var_list.shape[0],dtype='U13')\n",
    "for n in range(var_list.shape[0]):\n",
    "    time_str[n] = var_list[n,0].strftime('%Y-%m-%d-%H')\n",
    "    \n",
    "np.savetxt('/data/willytsai/ISCCP/time_event_2014_2018_10deg_4ds.dat',time_str,fmt='%13s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save dataset\n",
    "np.savetxt('/data/willytsai/ISCCP/var_event_2014_2018_10deg_4ds.dat',var_list[:,1:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
